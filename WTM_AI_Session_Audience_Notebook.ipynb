{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WTM AI Session - Audience Notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veere/WTM_AI_Bootcamp/blob/master/WTM_AI_Session_Audience_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wKR6bZwO4M90",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Learning AI Development with UX (Dec. 8 2018)\n"
      ]
    },
    {
      "metadata": {
        "id": "f_rnfzD_a5UC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Welcome to WTM and GDG Bangalore's session on \"Learning AI  Development with UX\". This notebook is your primary source for reference and place of work for all hands-on aspects of the workshop. \n",
        "\n",
        "The code samples and exercises in this notebook are desgined to run in Google's Colab Environment, while the same can be run on any machine that has python and dependencies for Keras, and Tensorflow installed. "
      ]
    },
    {
      "metadata": {
        "id": "nuhdNNtq4M92",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code snippets have been adapted from various tutorials and notebooks by Googlers and other researchers, to present a curated guide that covers the following aspects of AI development: \n",
        "* Dataset collection\n",
        "* Transforming raw data to features\n",
        "* Choosing a good model architecture\n",
        "* Building and Training an ML Model\n",
        "* Evaluating model performance and parameter tuning\n",
        "\n",
        "----\n",
        "## Text classification - A cookbook\n",
        "\n",
        "The main use case we will be following in this session is centered around Text Classification problems. While we will be alluding to various aspects of AI development mainly from the context of Text Classification, these concepts can be applied to other domains and problems as well. \n",
        "\n",
        "## What can you expect?\n",
        "\n",
        "As this is an end-to-end tutorial, we will be starting with collecting a dataset, and proceed to preprocessing input data to make it suitable for applying ML Algorithms, and then we will look at how to train neural networks with this preprocessed data, and evalutate our model's performance. We will conclude with a note on applying these concepts to other domain problems, and real world scenarios.  Lets get started!\n"
      ]
    },
    {
      "metadata": {
        "id": "Ju8F89y1eSXr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "\n",
        "As this is a hands-on workshop, we will be using Python as our programming language of choice, due to the vast number of libraries available for AI development, and its demonstrated ease of use for rapid prototyping. We will be building our models using [`keras`](https://keras.io/), a popular framework for working on Machine Learning problems, and we will be using [`tensorflow`](https://www.tensorflow.org/) as our backend for `keras`, to run our computations.  Don't worry if any of this sounds unfamiliar to you, as you will be able to pick it up as we go along. \n",
        "\n",
        "For those of you trying this out on your local computers, you may need to install `python3`  or `python2.7` along with `keras` and `tensorflow` -  both of which can be installed using pip. \n",
        "\n",
        "If you are running this tutorial in Google's Colab environment, all of these dependencies are installed for you out of the box. Let's start by confirming that we have `keras` installed and available. Run the following snippet to check what version of `keras` is installed here."
      ]
    },
    {
      "metadata": {
        "id": "41p941pj4M9v",
        "colab_type": "code",
        "outputId": "51ca7076-ecb4-4f9d-bd2b-dcfbe11ffa7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "EqOHB_Qk4M97",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Problem \n",
        "\n",
        "The basic premise of this hands-on session is Text-Classification. Text Classification in itself is a broad problem area that basically deals with categorizing textual data into one or more categories, depending on the objective and context. Some popular examples of Text Classification problems are: \n",
        "\n",
        "* Classifying emails into 'Spam' or 'Not Spam'\n",
        "* Classifying social media posts as 'Hate Speech' or 'Abusive'\n",
        "* Classifying product/movie reviews as 'Positive' or 'Negative'\n",
        "\n",
        "The above examples are that of Binary Classification problems, where the targets/labels are of either one kind or the other. However, Text Classification problems are not only limited to such cases, and we can have classification problems that can have multiple target candidates (more than 2). For example, consider the following cases: \n",
        "\n",
        "* Classifying customer queries into topic categories \n",
        "* Classifying tweets about the weather by time, sentiment, and kind\n",
        "* Multi-label classification of news article: eg assiging labels such as \"crime\", \"finance\" to an article on financial crimes.\n",
        "\n",
        "For the purpose of this hands-on session, we will look at classifying movie reviews into \"positive\" reviews and \"negative\" reviews, just based on the text content of the reviews. While this problem is fairly simple, it is a good starting point to understand the stages of development of an AI Solution, and the techniques we use are transferrable to a whole range of domains and problems. \n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "Goaskd0o4M-F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "Let's consider IMDB movie review sentiment prediction task that you may are already be familiar with. Let's quickly prepare the dataset."
      ]
    },
    {
      "metadata": {
        "id": "2Ea0RyGOa1M0",
        "colab_type": "code",
        "outputId": "71b1a05e-1ed3-4be8-8258-f59861a7641f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('Hello World!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "10f97m3U4M_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the IMDB data as raw text\n",
        "\n",
        "\n",
        "First, head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just \n",
        "Google \"IMDB dataset\"). Uncompress it."
      ]
    },
    {
      "metadata": {
        "id": "dQc75IX8mjMU",
        "colab_type": "code",
        "outputId": "a871cd1a-cbb2-445c-b9d0-ee0f2bc0eac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/arjun-rao/talks/raw/master/imdb.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-08 08:07:00--  https://github.com/arjun-rao/talks/raw/master/imdb.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/arjun-rao/talks/master/imdb.tar.gz [following]\n",
            "--2018-12-08 08:07:00--  https://raw.githubusercontent.com/arjun-rao/talks/master/imdb.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14806530 (14M) [application/octet-stream]\n",
            "Saving to: ‘imdb.tar.gz’\n",
            "\n",
            "\rimdb.tar.gz           0%[                    ]       0  --.-KB/s               \rimdb.tar.gz         100%[===================>]  14.12M  89.2MB/s    in 0.2s    \n",
            "\n",
            "2018-12-08 08:07:00 (89.2 MB/s) - ‘imdb.tar.gz’ saved [14806530/14806530]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDdywUm_3ovj",
        "colab_type": "code",
        "outputId": "92cbf2cf-85fc-46d0-c047-2bccfbdadcaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/arjun-rao/talks/raw/master/imdb-test.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-08 08:07:04--  https://github.com/arjun-rao/talks/raw/master/imdb-test.tar.gz\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/arjun-rao/talks/master/imdb-test.tar.gz [following]\n",
            "--2018-12-08 08:07:04--  https://raw.githubusercontent.com/arjun-rao/talks/master/imdb-test.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14485124 (14M) [application/octet-stream]\n",
            "Saving to: ‘imdb-test.tar.gz’\n",
            "\n",
            "imdb-test.tar.gz    100%[===================>]  13.81M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-12-08 08:07:04 (92.8 MB/s) - ‘imdb-test.tar.gz’ saved [14485124/14485124]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "27KyFwiH3xKH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xzf imdb.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1QgkZlom8Xe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xzf imdb-test.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DxMe5Videg9G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's collect the individual training reviews into a list of strings, one string per review, and let's also collect the review labels \n",
        "(positive / negative) into a `labels` list:"
      ]
    },
    {
      "metadata": {
        "id": "bE_tRC-34M_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_imdb(imdb_dir):\n",
        "  \"\"\"Loads the imdb dataset from the given path\"\"\"\n",
        "  train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "  labels = []\n",
        "  texts = []\n",
        "\n",
        "  for label_type in ['neg', 'pos']:\n",
        "      dir_name = os.path.join(train_dir, label_type)\n",
        "      for fname in os.listdir(dir_name):\n",
        "          if fname[-4:] == '.txt':\n",
        "              f = open(os.path.join(dir_name, fname))\n",
        "              texts.append(f.read())\n",
        "              f.close()\n",
        "              if label_type == 'neg':\n",
        "                  labels.append(0)\n",
        "              else:\n",
        "                  labels.append(1)\n",
        "  return texts, labels\n",
        "\n",
        "texts, labels = load_imdb('./')                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T45RUjC8bkOE",
        "colab_type": "code",
        "outputId": "a65bdf2f-165e-4ac6-f260-f2194a375fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(labels[0], texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, \"In order to avoid confusion, let me clarify a couple of points: I am not a red neck. I am not even a moderate nor a conservative. Quite on the contrary, I am a radical: a Libertarian. I'm not a WASP either, I was not even born in the States.<br /><br />Jorge Luis Borges used to say that there are some kind of folk who do not feel poetry, and that these sad people usually earn their living teaching poetry. This movie was made by and for people who do not feel poetry, by and for show-offs; and I dare say, by and for people who have no sense of decency or, for that matter, respect for other people's life or death (especially when the victims are thought to be mostly 'bloody imperialists' killed in Yankee soil.) I even find the original marketing idea of the eleven episodes of eleven minutes, nine seconds and one frame as particularly hideous and repulsive. Just plain awful. Why didn't they assign a budget of as many dollars per episode as individuals were brutally murdered in the attack? The whole idea rests somewhere between mere stupidity and reckless fascism. Anybody who is serious about film-making (and serious about life and death) should have angrily declined to participate in this recollection of innuendoes and non-sequiturs. With two exceptions: the episode of Burkina Faso -- almost amusing --, and the one from India --which documents the story of a man who was unfairly and wrongly investigated in relation to the attack, on the basis that afterwards he didn't return home and that he was an American Muslim (and, truth be told, when the facts were known he was honored as a hero). All other nine episodes, essentially and extremely boring and emotionless, can be listed in two different categories:<br /><br />First: 'I don't care about the thousands of victims: Americans, foreigners, children, youngsters, adults, old-timers...' and can be resumed in pure boredom and lack of emotion. Makhmalbaf's (Iran); Lelouch's (France) \\xc2\\x96 I'm afraid I'm going to commit an heresy since it's Lelouch's, but maybe, his episode might be considered built upon an idea which could be regarded as almost original; Tanovic's (Bosnia-Herzegovina); Gonzalez Inarritu's (Mexico); Gita\\xc3\\xaf's (Israel); Penn's (USA) <br /><br />Second: 'The bloody Yankees deserve it'. And can be resumed in frustration and hatred. Chahine (Egypt) vindicates the suicide bombers; Loach (UK) considers the 9/11 reckless attacks were some kind of punishment for the alleged support of the USA to the Chilean dictatorship headed by the serial-killer Augusto Pinochet, in fact someone should inform Mr. Loach that the victims of Pinochet were not related to Al-Qaida and that Chile is a South American country which sole existence Mr. Bin Laden should have ignored, he ought to be informed too that the American government sanctions against the Chilean dictatorship were harder than any other ciountrie's; and, Imamura (Japan) windingly points out that WWII is related the attack to the WTC. Imamura has at least been coherent in this: the supposed cause effect linking is entirely nonsensical, which plays well with his episode including a man who believes himself to be a snake. It pretends to be obscure. It is, instead, quite ludicrous.<br /><br />There's some kind of error shared by many, including some Americans, and it consists in the belief that this movie wasn't commercially screened in the States because of some kind of censorship. Nothing further from the truth: This movie wasn't screened in the States because it is a complete fiasco. A fiasco of the wackyest kind. Even in Buenos Aires, where Peronism and other forms of Fascism are nearest and dearest to the hearts of a sizable number of its inhabitants, and anti-Americanism is in vogue, the movie was screened in living rooms hurriedly converted into theaters, and was applauded by a very select public: The usual sad few who routinely lend their applause to other equally 'quaint' spectacles. Like the sight of a McDonald's fast-food restaurant or, perchance, an elderly Jew, being burnt to ashes.\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FW7tzJb4M_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenize the data\n",
        "\n",
        "\n",
        "Let's vectorize the texts we collected. To do this we will also need some popular functions from nltk, so let's load them first."
      ]
    },
    {
      "metadata": {
        "id": "NIoZSP-E0cRD",
        "colab_type": "code",
        "outputId": "46ab9b5b-9300-4394-de69-04893dcbef43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection u'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "wfhJ4wGtcoXa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-EQY7Rrwcr9O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "?stopwords.words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MdbofqVo4M_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "def preprocess_stopwords(texts):\n",
        "  \"\"\"Returns a list of texts with the stopwords removed\"\"\"\n",
        "  result = []\n",
        "  \n",
        "  #Load the set of stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  \n",
        "  #Split each sentence into words, retain only words in the sentence that are not in the stopwords list\n",
        "  for sentence in texts:\n",
        "    tokens = sentence.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    result.append(' '.join(filtered_tokens))\n",
        "  return result\n",
        "\n",
        "def tokenize_texts(texts, labels, max_len, max_words, remove_stopwords=True):\n",
        "  \"\"\"Returns tokenized data according to given params\"\"\"\n",
        "  data = []\n",
        "  labels = np.asarray(labels)\n",
        "  tokenizer = None\n",
        "  \n",
        "    \n",
        "  # Initialise Tokenizer with maxwords, fit the tokenizer to the texts and use \n",
        "  # the tokenizer to obtain sequences from text\n",
        "  #Reference - https://faroit.github.io/keras-docs/2.0.2/preprocessing/text/#tokenizer\n",
        "  tokenizer = Tokenizer(num_words=max_words)\n",
        "  tokenizer.fit_on_texts(texts)\n",
        "  sequences = tokenizer.texts_to_sequences(texts)\n",
        "  \n",
        "  #Pad the sequences to obtain vectors of 'max_len'\n",
        "  #https://faroit.github.io/keras-docs/2.0.2/preprocessing/sequence/#pad_sequences\n",
        "  data = pad_sequences(sequences, maxlen=max_len)\n",
        "  return data, labels, tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nY2mICAJwvmx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split the tokenized data into Train and Validation sets"
      ]
    },
    {
      "metadata": {
        "id": "CgVw8XDrwwzM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_data(data, labels, train_size=20000, validation_size=5000):\n",
        "  \"\"\"Returns train and validation data and corresponding labels\"\"\"\n",
        "  x_train = []\n",
        "  y_train = []\n",
        "  x_validation = []\n",
        "  y_validation = []\n",
        "  labels = np.asarray(labels)\n",
        "  \n",
        "  # Shuffle the data, since we started from data where samples are ordered \n",
        "  # all negative first, then all positive\n",
        "  # Make sure you shuffle the data and labels in the same order \n",
        "  indices = np.arange(data.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  data = data[indices]\n",
        "  labels = labels[indices]\n",
        "  \n",
        "  # Split data and labels into train and validation\n",
        "  x_train = data[:train_size]\n",
        "  y_train = labels[:train_size]\n",
        "  x_validation = data[train_size: train_size + validation_size]\n",
        "  y_validation = labels[train_size: train_size + validation_size]\n",
        "  return ((x_train, y_train), (x_validation, y_validation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-Tk9VMMfhXNI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "?np.arange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "646nmWTO6gvX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll tokenize our text to have 20 words per review, and train on 20000 reviews, and test on 5000 reviews. We'll also restrict our tokenizer to top 10000 words across reviews."
      ]
    },
    {
      "metadata": {
        "id": "npEB-9C_wgZ8",
        "colab_type": "code",
        "outputId": "93ef7a8a-2605-4d83-922d-5060baca0d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "data, labels, tokenizer = tokenize_texts(texts, labels, 20, 10000, remove_stopwords=False)\n",
        "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "((x_train, y_train), (x_val, y_val)) = split_data(data, labels, train_size=20000, validation_size=5000)\n",
        "print('Shape of train data:', x_train.shape)\n",
        "print('Shape of train labels:', y_train.shape)\n",
        "print('Shape of validation data:', x_val.shape)\n",
        "print('Shape of validation labels:', y_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 88584 unique tokens.\n",
            "('Shape of data tensor:', (25000, 20))\n",
            "('Shape of label tensor:', (25000,))\n",
            "('Shape of train data:', (20000, 20))\n",
            "('Shape of train labels:', (20000,))\n",
            "('Shape of validation data:', (5000, 20))\n",
            "('Shape of validation labels:', (5000,))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LHG28J3qfKTt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see what we just did: \n",
        "* We first split the sentences into tokens\n",
        "* Then we store these tokens into an index\n",
        "* Then we convert our sentences into a sequence of integers, where each integer is an id for the corresponding word in our index. \n",
        "\n",
        "Let's take a look at the results for an example:"
      ]
    },
    {
      "metadata": {
        "id": "zp60MI7Jee9C",
        "colab_type": "code",
        "outputId": "6be87e35-8e66-458d-ca06-2a964b4ff590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "print(texts[0])\n",
        "print(preprocess_stopwords([texts[0]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In order to avoid confusion, let me clarify a couple of points: I am not a red neck. I am not even a moderate nor a conservative. Quite on the contrary, I am a radical: a Libertarian. I'm not a WASP either, I was not even born in the States.<br /><br />Jorge Luis Borges used to say that there are some kind of folk who do not feel poetry, and that these sad people usually earn their living teaching poetry. This movie was made by and for people who do not feel poetry, by and for show-offs; and I dare say, by and for people who have no sense of decency or, for that matter, respect for other people's life or death (especially when the victims are thought to be mostly 'bloody imperialists' killed in Yankee soil.) I even find the original marketing idea of the eleven episodes of eleven minutes, nine seconds and one frame as particularly hideous and repulsive. Just plain awful. Why didn't they assign a budget of as many dollars per episode as individuals were brutally murdered in the attack? The whole idea rests somewhere between mere stupidity and reckless fascism. Anybody who is serious about film-making (and serious about life and death) should have angrily declined to participate in this recollection of innuendoes and non-sequiturs. With two exceptions: the episode of Burkina Faso -- almost amusing --, and the one from India --which documents the story of a man who was unfairly and wrongly investigated in relation to the attack, on the basis that afterwards he didn't return home and that he was an American Muslim (and, truth be told, when the facts were known he was honored as a hero). All other nine episodes, essentially and extremely boring and emotionless, can be listed in two different categories:<br /><br />First: 'I don't care about the thousands of victims: Americans, foreigners, children, youngsters, adults, old-timers...' and can be resumed in pure boredom and lack of emotion. Makhmalbaf's (Iran); Lelouch's (France)  I'm afraid I'm going to commit an heresy since it's Lelouch's, but maybe, his episode might be considered built upon an idea which could be regarded as almost original; Tanovic's (Bosnia-Herzegovina); Gonzalez Inarritu's (Mexico); Gitaï's (Israel); Penn's (USA) <br /><br />Second: 'The bloody Yankees deserve it'. And can be resumed in frustration and hatred. Chahine (Egypt) vindicates the suicide bombers; Loach (UK) considers the 9/11 reckless attacks were some kind of punishment for the alleged support of the USA to the Chilean dictatorship headed by the serial-killer Augusto Pinochet, in fact someone should inform Mr. Loach that the victims of Pinochet were not related to Al-Qaida and that Chile is a South American country which sole existence Mr. Bin Laden should have ignored, he ought to be informed too that the American government sanctions against the Chilean dictatorship were harder than any other ciountrie's; and, Imamura (Japan) windingly points out that WWII is related the attack to the WTC. Imamura has at least been coherent in this: the supposed cause effect linking is entirely nonsensical, which plays well with his episode including a man who believes himself to be a snake. It pretends to be obscure. It is, instead, quite ludicrous.<br /><br />There's some kind of error shared by many, including some Americans, and it consists in the belief that this movie wasn't commercially screened in the States because of some kind of censorship. Nothing further from the truth: This movie wasn't screened in the States because it is a complete fiasco. A fiasco of the wackyest kind. Even in Buenos Aires, where Peronism and other forms of Fascism are nearest and dearest to the hearts of a sizable number of its inhabitants, and anti-Americanism is in vogue, the movie was screened in living rooms hurriedly converted into theaters, and was applauded by a very select public: The usual sad few who routinely lend their applause to other equally 'quaint' spectacles. Like the sight of a McDonald's fast-food restaurant or, perchance, an elderly Jew, being burnt to ashes.\n",
            "[\"In order avoid confusion, let clarify couple points: I red neck. I even moderate conservative. Quite contrary, I radical: Libertarian. I'm WASP either, I even born States.<br /><br />Jorge Luis Borges used say kind folk feel poetry, sad people usually earn living teaching poetry. This movie made people feel poetry, show-offs; I dare say, people sense decency or, matter, respect people's life death (especially victims thought mostly 'bloody imperialists' killed Yankee soil.) I even find original marketing idea eleven episodes eleven minutes, nine seconds one frame particularly hideous repulsive. Just plain awful. Why assign budget many dollars per episode individuals brutally murdered attack? The whole idea rests somewhere mere stupidity reckless fascism. Anybody serious film-making (and serious life death) angrily declined participate recollection innuendoes non-sequiturs. With two exceptions: episode Burkina Faso -- almost amusing --, one India --which documents story man unfairly wrongly investigated relation attack, basis afterwards return home American Muslim (and, truth told, facts known honored hero). All nine episodes, essentially extremely boring emotionless, listed two different categories:<br /><br />First: 'I care thousands victims: Americans, foreigners, children, youngsters, adults, old-timers...' resumed pure boredom lack emotion. Makhmalbaf's (Iran); Lelouch's (France) \\xc2\\x96 I'm afraid I'm going commit heresy since Lelouch's, maybe, episode might considered built upon idea could regarded almost original; Tanovic's (Bosnia-Herzegovina); Gonzalez Inarritu's (Mexico); Gita\\xc3\\xaf's (Israel); Penn's (USA) <br /><br />Second: 'The bloody Yankees deserve it'. And resumed frustration hatred. Chahine (Egypt) vindicates suicide bombers; Loach (UK) considers 9/11 reckless attacks kind punishment alleged support USA Chilean dictatorship headed serial-killer Augusto Pinochet, fact someone inform Mr. Loach victims Pinochet related Al-Qaida Chile South American country sole existence Mr. Bin Laden ignored, ought informed American government sanctions Chilean dictatorship harder ciountrie's; and, Imamura (Japan) windingly points WWII related attack WTC. Imamura least coherent this: supposed cause effect linking entirely nonsensical, plays well episode including man believes snake. It pretends obscure. It is, instead, quite ludicrous.<br /><br />There's kind error shared many, including Americans, consists belief movie commercially screened States kind censorship. Nothing truth: This movie screened States complete fiasco. A fiasco wackyest kind. Even Buenos Aires, Peronism forms Fascism nearest dearest hearts sizable number inhabitants, anti-Americanism vogue, movie screened living rooms hurriedly converted theaters, applauded select public: The usual sad routinely lend applause equally 'quaint' spectacles. Like sight McDonald's fast-food restaurant or, perchance, elderly Jew, burnt ashes.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i6AgS3sQyd9i",
        "colab_type": "code",
        "outputId": "bf7bba44-df78-49c3-b611-5f4e71aff61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#Ouput the padded sequence for texts[0]\n",
        "tokens = tokenizer.texts_to_sequences([texts[0]])\n",
        "tokens = pad_sequences(tokens, maxlen=60)\n",
        "print(tokens[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4287    4   23 8423    2    5    1 3366    4    3  608    4   91 5794\n",
            "    2 1207    6    8    1   17   13 7917    8  578 4311 9259   80 2253\n",
            "    2   13   31    3   52 1067    1  641  614  168   34 6259   65 8598\n",
            "    5   82 1301   37    1 1679    4    3  699 1640 3734   39   32 3714\n",
            " 9092  109 7262    5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YNsfMx0Yz3qK",
        "colab_type": "code",
        "outputId": "5f9ffda1-4c2d-480a-f571-9380a2e3d35a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#Output the tokens from the padded sequence\n",
        "print([tokenizer.index_word[token] for token in tokens[0] if token])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['forms', 'of', 'are', 'nearest', 'and', 'to', 'the', 'hearts', 'of', 'a', 'number', 'of', 'its', 'inhabitants', 'and', 'anti', 'is', 'in', 'the', 'movie', 'was', 'screened', 'in', 'living', 'rooms', 'converted', 'into', 'theaters', 'and', 'was', 'by', 'a', 'very', 'public', 'the', 'usual', 'sad', 'few', 'who', 'lend', 'their', 'applause', 'to', 'other', 'equally', 'like', 'the', 'sight', 'of', 'a', 'fast', 'food', 'restaurant', 'or', 'an', 'elderly', 'jew', 'being', 'burnt', 'to']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jqkLHZgv8oi5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings (also called Word Vectors)\n",
        "\n",
        "Before we can train our fancy ML model, lets look at how we can take these sequences of integers and derive some representation that is more meaningful and conveys more information to our models. This is where word embeddings come in. \n",
        "\n",
        "Word vectors or embeddings are simply vectors of numbers that represent the meaning of a word in some context. Why do we need word vectors? Our original sequence of word indices do not capture information about a word’s meaning or context. \n",
        "\n",
        "This means that potential relationships, such as contextual closeness, are not captured across collections of words in our original sequence. For example, a one-hot encoding from this sequence cannot capture simple relationships, such as determining that the words “dog” and “cat” both refer to animals that are often discussed in the context of household pets.\n",
        "\n",
        "Word vectors represent words as multidimensional continuous floating point numbers where semantically similar words are mapped to proximate points in geometric space. \n",
        "\n",
        "In simpler terms, a word vector is a row of real valued numbers (as opposed to dummy numbers) where each point captures a dimension of the word’s meaning and where semantically similar words have similar vectors. \n",
        "\n",
        "This means that words such as wheel and engine should have similar word vectors to the word car (because of the similarity of their meanings), whereas the word banana should be quite distant. \n",
        "\n",
        "Put differently, words that are used in a similar context will be mapped to a proximate vector space (we will get to how these word vectors are created below). The beauty of representing words as vectors is that they lend themselves to mathematical operators. \n",
        "\n",
        "For example, we can add and subtract vectors — the canonical example here is showing that by using word vectors we can determine that:\n",
        "\n",
        "king — man + woman = queen\n",
        "\n",
        "In other words, we can subtract one meaning from the word vector for king (i.e. maleness), add another meaning (femaleness), and show that this new word vector (king — man + woman) maps most closely to the word vector for queen.\n",
        "\n",
        "The numbers in the word vector represent the word’s distributed weight across dimensions. In a simplified sense each dimension represents a meaning and the word’s numerical weight on that dimension captures the closeness of its association with and to that meaning. Thus, the semantics of the word are embedded across the dimensions of the vector.\n",
        "\n",
        "## Example for Word Embeddings\n",
        "\n",
        "Let's see this in action - We'll load a pre-trainied model for word vectors using `spacy` a popular NLP library - and visualize some word vectors using PCA. \n",
        "\n",
        "Start by downloading the `en` spacy model to get the pretrained vectors."
      ]
    },
    {
      "metadata": {
        "id": "iGEAucfowYqO",
        "colab_type": "code",
        "outputId": "20be6fe7-9cc9-4111-af5a-fe19e66d4bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 86.8MB/s \n",
            "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python2.7/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python2.7/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ORfgxsVBdfxT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try and use Spacy's pretrained word vectors to visualize a set of words transformed into word embeddings and reduced to 2 dimensions using PCA."
      ]
    },
    {
      "metadata": {
        "id": "CYSQWpPE96Yx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.decomposition import PCA\n",
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rwOrjor41K3X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words = u\"man king woman queen palace royal house\"\n",
        "words_tokens = nlp(words)\n",
        "words_vectors = np.vstack([word.vector for word in words_tokens if word.has_vector])\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_transformed = pca.fit_transform(words_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4qRvZ5SKwFYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = np.array([item[0] for item in word_vectors_transformed])\n",
        "y = np.array([item[1] for item in word_vectors_transformed])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BQXo1qHwoxLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### So how does an embedding look like? \n",
        "\n",
        "From the above example, the embedding for the word `man` is: \n"
      ]
    },
    {
      "metadata": {
        "id": "-yKSlH1-pATq",
        "colab_type": "code",
        "outputId": "71d525e0-ae7d-4aef-b6a1-0175888aef15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "words_tokens[0],words_vectors[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(man, array([[ 5.6654114e-01,  2.1298788e+00, -7.5750500e-01, ...,\n",
              "         -4.4764426e-01,  1.5666524e-01, -2.8439707e-01],\n",
              "        [-3.8338602e-03, -1.7044394e+00,  1.1055291e+00, ...,\n",
              "         -5.2565366e-02,  2.1175520e-01, -3.5021394e-02],\n",
              "        [ 1.2737572e-02, -2.4252005e+00,  8.0763125e-01, ...,\n",
              "          7.3892012e-02, -9.0038776e-04, -2.3811609e-02],\n",
              "        ...,\n",
              "        [ 5.9697354e-01,  2.0848435e-01, -2.4370432e+00, ...,\n",
              "         -7.2301191e-01, -3.9200917e-01,  3.9495468e-01],\n",
              "        [ 1.5074633e-01,  8.2599211e-01,  6.4324105e-01, ...,\n",
              "         -5.8896923e-01,  4.8564866e-02,  1.3803323e-01],\n",
              "        [ 2.3954535e+00,  2.3134739e+00,  3.0796623e+00, ...,\n",
              "         -3.8401619e-01, -7.3281050e-01,  3.0168864e-01]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "j7NSeW-aZxCF",
        "colab_type": "code",
        "outputId": "a5a80018-cb37-44cb-f1a6-eca8ed5c0735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(\n",
        "    {'x': x,\n",
        "     'y': y,\n",
        "     'txt': words.split()\n",
        "    })\n",
        "\n",
        "# plot the dataset, referencing dataframe column names\n",
        "import altair as alt\n",
        "base = alt.Chart(df).encode(\n",
        "    x='x', \n",
        "    y='y'\n",
        ")\n",
        "\n",
        "base.mark_circle(size=100) + base.mark_text(dx=15).encode(text='txt').interactive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LayerChart({\n",
              "  layer: [Chart({\n",
              "    data:       txt          x          y\n",
              "    0     man  14.015564   4.079728\n",
              "    1    king   3.999183   3.894140\n",
              "    2   woman   8.001021   0.507969\n",
              "    3   queen  -6.139023  -6.126812\n",
              "    4  palace   2.822722  -7.135913\n",
              "    5   royal  -9.862520 -11.066113\n",
              "    6   house -12.836949  15.846996,\n",
              "    encoding: EncodingWithFacet({\n",
              "      x: X({\n",
              "        shorthand: 'x'\n",
              "      }),\n",
              "      y: Y({\n",
              "        shorthand: 'y'\n",
              "      })\n",
              "    }),\n",
              "    mark: MarkDef({\n",
              "      size: 100,\n",
              "      type: 'circle'\n",
              "    })\n",
              "  }), Chart({\n",
              "    data:       txt          x          y\n",
              "    0     man  14.015564   4.079728\n",
              "    1    king   3.999183   3.894140\n",
              "    2   woman   8.001021   0.507969\n",
              "    3   queen  -6.139023  -6.126812\n",
              "    4  palace   2.822722  -7.135913\n",
              "    5   royal  -9.862520 -11.066113\n",
              "    6   house -12.836949  15.846996,\n",
              "    encoding: EncodingWithFacet({\n",
              "      text: Text({\n",
              "        shorthand: 'txt'\n",
              "      }),\n",
              "      x: X({\n",
              "        shorthand: 'x'\n",
              "      }),\n",
              "      y: Y({\n",
              "        shorthand: 'y'\n",
              "      })\n",
              "    }),\n",
              "    mark: MarkDef({\n",
              "      dx: 15,\n",
              "      type: 'text'\n",
              "    }),\n",
              "    selection: SelectionMapping({\n",
              "      selector001: SelectionDef({\n",
              "        bind: 'scales',\n",
              "        encodings: ['x', 'y'],\n",
              "        type: 'interval'\n",
              "      })\n",
              "    })\n",
              "  })]\n",
              "})"
            ],
            "text/html": [
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "  <style>\n",
              "    .vega-actions a {\n",
              "        margin-right: 12px;\n",
              "        color: #757575;\n",
              "        font-weight: normal;\n",
              "        font-size: 13px;\n",
              "    }\n",
              "    .error {\n",
              "        color: red;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega@3.3.1\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega-lite@2.6.0\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm//vega-embed@3.14\"></script>\n",
              "\n",
              "</head>\n",
              "<body>\n",
              "  <div id=\"vis\"></div>\n",
              "  <script type=\"text/javascript\">\n",
              "    var spec = {\"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 100}, \"data\": {\"name\": \"data-4720a05e582e4750c7de87ecc05d77b6\"}, \"encoding\": {\"y\": {\"field\": \"y\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"x\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"text\", \"dx\": 15}, \"selection\": {\"selector001\": {\"bind\": \"scales\", \"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}}, \"data\": {\"name\": \"data-4720a05e582e4750c7de87ecc05d77b6\"}, \"encoding\": {\"y\": {\"field\": \"y\", \"type\": \"quantitative\"}, \"text\": {\"field\": \"txt\", \"type\": \"nominal\"}, \"x\": {\"field\": \"x\", \"type\": \"quantitative\"}}}], \"datasets\": {\"data-4720a05e582e4750c7de87ecc05d77b6\": [{\"y\": 4.079727649688721, \"x\": 14.01556396484375, \"txt\": \"man\"}, {\"y\": 3.8941402435302734, \"x\": 3.999183416366577, \"txt\": \"king\"}, {\"y\": 0.5079692602157593, \"x\": 8.001021385192871, \"txt\": \"woman\"}, {\"y\": -6.126811981201172, \"x\": -6.139023303985596, \"txt\": \"queen\"}, {\"y\": -7.135913372039795, \"x\": 2.8227221965789795, \"txt\": \"palace\"}, {\"y\": -11.066112518310547, \"x\": -9.862520217895508, \"txt\": \"royal\"}, {\"y\": 15.846996307373047, \"x\": -12.836949348449707, \"txt\": \"house\"}]}, \"config\": {\"view\": {\"width\": 400, \"height\": 300}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v2.6.0.json\"};\n",
              "    var embed_opt = {\"mode\": \"vega-lite\"};\n",
              "\n",
              "    function showError(el, error){\n",
              "        el.innerHTML = ('<div class=\"error\">'\n",
              "                        + '<p>JavaScript Error: ' + error.message + '</p>'\n",
              "                        + \"<p>This usually means there's a typo in your chart specification. \"\n",
              "                        + \"See the javascript console for the full traceback.</p>\"\n",
              "                        + '</div>');\n",
              "        throw error;\n",
              "    }\n",
              "    const el = document.getElementById('vis');\n",
              "    vegaEmbed(\"#vis\", spec, embed_opt)\n",
              "      .catch(error => showError(el, error));\n",
              "  </script>\n",
              "</body>\n",
              "</html>\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "2DfLfq9gmrXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see the words `man` and `woman` are along the same direction and that of `king` and queen are along the same direction as that of `man` and `woman` so the vector direction from `man` -> `woman` probably denotes the semantic meaning of gender. \n",
        "\n",
        "Disclaimer:\n",
        "The PCA visualization is probably not the best way to represent the actual embeddings, but it is illustrative of the broad concept, and hence we have it here. Do not attempt to visualize your actual embeddings using this technique and hope to get some meaningful outcomes. You most probably will not.\n",
        "\n",
        "## Training a model for the IMDB Dataset\n",
        "\n",
        "Lets train a simple neural network for our IMDB Dataset and use an embedding layer that first transforms our word sequences into word vectors.\n",
        "\n",
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n",
        "\n",
        "* Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. These are called \"pre-trained word embeddings\".\n",
        "\n",
        "We'll first go with our first approach by learning the embeddings using the `Embedding` layer from `keras`.\n",
        "\n",
        "\n",
        "The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes \n",
        "as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup.\n",
        "\n",
        "The Embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15). All sequences in a batch must have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.\n",
        "\n",
        "This layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality). Such a 3D tensor can then be processed by a RNN layer or a 1D convolution layer.\n",
        "\n",
        "When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just like with any other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for."
      ]
    },
    {
      "metadata": {
        "id": "rTy2JNgdwVVF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Recap\n",
        "\n",
        "We generated our training set using the following block of code:"
      ]
    },
    {
      "metadata": {
        "id": "sEA4Da2WwZym",
        "colab_type": "code",
        "outputId": "2bdd5ccc-a025-4391-993e-2caeabcd1489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "texts, labels = load_imdb('./')\n",
        "data, labels, tokenizer = tokenize_texts(texts, labels, 20, 10000, remove_stopwords=False)\n",
        "((x_train, y_train), (x_val, y_val)) = split_data(data, labels, train_size=20000, validation_size=5000)\n",
        "print('Shape of train data:', x_train.shape)\n",
        "print('Shape of train labels:', y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Shape of train data:', (20000, 20))\n",
            "('Shape of train labels:', (20000,))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M1NCPwW9pD95",
        "colab_type": "code",
        "outputId": "f692a47b-6f53-49a8-f2fe-054bd4da67fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(x_val.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZFAiAcLJqvxz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build and train a Sequential model with the following specs:\n",
        "\n",
        "* Embedding Layer with maximum number of tokes to be 10000 and embedding dimensionality as 8. Let the input_length be the maximum length of each review i.e 20 as seen previously.\n",
        "* Flatten the 3D embedding output to 2D.\n",
        "* Dense Layer which is the classifier.\n",
        "* Compile the model with a 'rmsprop' optimizer. Can you guess what loss we need to use?\n",
        "* Let accuracy be one of the metrics we are interested in.\n",
        "* Run the model on the above training data. "
      ]
    },
    {
      "metadata": {
        "id": "VHekBnOZpmvE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dense\n",
        "?Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tdEk_THwphXA",
        "colab_type": "code",
        "outputId": "5d2ff570-891b-4c4d-c861-26a3557815af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding Layer with maximum number of tokens to be 10000 and embedding dimensionality as 8. \n",
        "# Let the input_length be the maximum length of each review i.e 20 as seen previously.\n",
        "# Refernece - https://keras.io/layers/embeddings/\n",
        "model.add(Embedding(10000, 8, input_length=20))\n",
        "\n",
        "# After the Embedding layer, our activations have shape `(samples, maxlen, 8)`.\n",
        "# We flatten the 3D tensor of embeddings into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "# Reference - https://keras.io/layers/core/#flatten\n",
        "model.add(Flatten())\n",
        "\n",
        "# We add a Dense classifier on top with a sigmoid activation\n",
        "# Reference - https://keras.io/layers/core/#dense\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model with a 'rmsprop' optimizer. Can you guess what loss we need to use?\n",
        "# Let accuracy be one of the metrics we are interested in.\n",
        "# Reference - https://keras.io/models/model/#compile\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LfLPrUqQ7sqm",
        "colab_type": "code",
        "outputId": "43cb07c5-c2b8-4b41-bfd4-2ace72c06211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "# Run the model on the above training data for 10 epochs and batch size of 32\n",
        "# Pass the validation data to evaluate the model\n",
        "# Reference- https://keras.io/models/model/#fit\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 2s 76us/step - loss: 0.6776 - acc: 0.5982 - val_loss: 0.6338 - val_acc: 0.6946\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.5563 - acc: 0.7435 - val_loss: 0.5236 - val_acc: 0.7436\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.4640 - acc: 0.7882 - val_loss: 0.4897 - val_acc: 0.7626\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.4174 - acc: 0.8132 - val_loss: 0.4820 - val_acc: 0.7676\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.3859 - acc: 0.8294 - val_loss: 0.4823 - val_acc: 0.7692\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 1s 68us/step - loss: 0.3606 - acc: 0.8432 - val_loss: 0.4875 - val_acc: 0.7690\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.3390 - acc: 0.8546 - val_loss: 0.4947 - val_acc: 0.7648\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.3188 - acc: 0.8655 - val_loss: 0.5032 - val_acc: 0.7652\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.3006 - acc: 0.8744 - val_loss: 0.5130 - val_acc: 0.7640\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 1s 67us/step - loss: 0.2831 - acc: 0.8845 - val_loss: 0.5233 - val_acc: 0.7638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rItwEjljto-A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The resulting model accuracy is rather low. Let's see if we can improve this by tweaking a few parameters in our model. We'll first increase the number of words we consider.\n"
      ]
    },
    {
      "metadata": {
        "id": "qDYgaMDl0bDh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the dataset again\n",
        "#Tokenize the data, this time passing max_len as 200\n",
        "#Split the data into train and validation\n",
        "texts, labels = load_imdb('./')  \n",
        "data, labels, tokenizer = tokenize_texts(texts, labels, 200, 10000, remove_stopwords=True)\n",
        "((x_train, y_train), (x_val, y_val)) = split_data(data, labels, train_size=20000, validation_size=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NDWfdvRtwmU",
        "colab_type": "code",
        "outputId": "8f4db39c-c299-4432-b26d-1f0514783254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "cell_type": "code",
      "source": [
        "#Train a similar model as the previous one\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.layers import Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=200))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 200, 8)            80000     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 1601      \n",
            "=================================================================\n",
            "Total params: 81,601\n",
            "Trainable params: 81,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 2s 88us/step - loss: 0.5889 - acc: 0.7040 - val_loss: 0.4050 - val_acc: 0.8414\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 2s 76us/step - loss: 0.3103 - acc: 0.8798 - val_loss: 0.2988 - val_acc: 0.8804\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 2s 78us/step - loss: 0.2337 - acc: 0.9103 - val_loss: 0.2773 - val_acc: 0.8880\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 2s 76us/step - loss: 0.1975 - acc: 0.9238 - val_loss: 0.2776 - val_acc: 0.8888\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 2s 78us/step - loss: 0.1729 - acc: 0.9352 - val_loss: 0.2760 - val_acc: 0.8894\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 2s 78us/step - loss: 0.1531 - acc: 0.9429 - val_loss: 0.2847 - val_acc: 0.8826\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 2s 78us/step - loss: 0.1353 - acc: 0.9507 - val_loss: 0.2888 - val_acc: 0.8844\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 2s 78us/step - loss: 0.1192 - acc: 0.9582 - val_loss: 0.2970 - val_acc: 0.8844\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 2s 78us/step - loss: 0.1043 - acc: 0.9648 - val_loss: 0.3081 - val_acc: 0.8836\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 2s 78us/step - loss: 0.0908 - acc: 0.9707 - val_loss: 0.3227 - val_acc: 0.8802\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6tG9HrvO0vp0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With a higher number of words considered per review the validation accuracy can increase up to 88%."
      ]
    },
    {
      "metadata": {
        "id": "pmJ3cOCH5ziW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using embeddings: An illustration\n",
        "\n",
        "Let's try and use embeddings to make our model better. \n",
        "\n",
        "Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), we will add the following twist: we restrict the training data to its first 200 samples"
      ]
    },
    {
      "metadata": {
        "id": "ae5hTPLt51lG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the dataset again\n",
        "#Tokenize the data, passing max_len as 200\n",
        "#Split the data into train (size = 200)and validation (size=10000)\n",
        "\n",
        "\n",
        "#Output the shape of the new training data and labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CnJKOkgj4M_x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now let's build an embedding matrix that we will be able to load into an `Embedding` layer. It must be a matrix of shape `(max_words, embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
        "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder.\n",
        "\n",
        "We'll use spacy's word vectors to create our embedding layer. Lets start by getting the words from our tokenizer, and then transform them into their embeddings. While we use Spacy embeddings - you can instead use GloVe or Word2Vec as well. We chose spacy due to its smaller size for download and the purpose of illustration - not to get the best possible model."
      ]
    },
    {
      "metadata": {
        "id": "g0fWabGQ0taP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Obtain the tokenizer word index\n",
        "#Use the word index to retrieve all 10000 words\n",
        "all_words = [] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_pLaD8FML47l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NAWs0Lnc0fiV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en\")\n",
        "\n",
        "#Obtain word embedding using Spacy\n",
        "word_vectors = nlp(u' '.join(all_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7WibT3s7rSc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Obtain the embedding_dim\n",
        "embedding_dim = word_vectors[0].vector.shape[0]\n",
        "print(embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyG_Q1v37lEs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's construct our embedding matrix like so: "
      ]
    },
    {
      "metadata": {
        "id": "K1AAiH8o4M_1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_words = 10000\n",
        "embedding_dim = word_vectors[0].vector.shape[0]\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for i, word in enumerate(word_vectors):\n",
        "    embedding_vector = word.vector if word.has_vector else None\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DgUOUM61j4a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "btrbZIgQ4M_-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a model\n",
        "\n",
        "We will be using the same model architecture as before  with an additional Dense layer using `relu` activation:"
      ]
    },
    {
      "metadata": {
        "id": "_FjQxClv4NAA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a similar model as the previous one, with an additional Dense Layer using relu activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6gbJoH5U4NAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the embeddings in the model\n",
        "\n",
        "\n",
        "The `Embedding` layer has a single weight matrix: a 2D float matrix where each entry `i` is the word vector meant to be associated with \n",
        "index `i`. Simple enough. Let's just load the embedding matrix we prepared into our `Embedding` layer, the first layer in our model:"
      ]
    },
    {
      "metadata": {
        "id": "p-4GdS-H4NAD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DTaUSGXC4NAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Additionally, we freeze the embedding layer (we set its `trainable` attribute to `False`), following the same rationale as what you are \n",
        "already familiar with in the context of pre-trained ConvNet features: when parts of a model are pre-trained (like our `Embedding` layer), \n",
        "and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting \n",
        "what they already know. The large gradient update triggered by the randomly initialized layers would be very disruptive to the already \n",
        "learned features."
      ]
    },
    {
      "metadata": {
        "id": "Bz_UorNa4NAH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate\n",
        "\n",
        "Let's compile our model and train it:"
      ]
    },
    {
      "metadata": {
        "id": "NhTK_jkr4NAI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compile and train model\n",
        "\n",
        "#Save the model weights to a file\n",
        "model.save_weights('pre_trained_spacy_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "957OvDMC4NAK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot its performance over time:"
      ]
    },
    {
      "metadata": {
        "id": "6KKJM3ZF4NAL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BpvSIAW4NAO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The model quickly starts overfitting, unsurprisingly given the small number of training samples. Validation accuracy though seems to reach 50s.\n",
        "\n",
        "Note that your mileage may vary: since we have so few training samples, performance is heavily dependent on which exact 200 samples we \n",
        "picked, and we picked them at random. If it worked really poorly for you, try picking a different random set of 200 samples, just for the \n",
        "sake of the exercise (in real life you don't get to pick your training data).\n",
        "\n",
        "We can also try to train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. In that \n",
        "case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings \n",
        "when lots of data is available. However, in our case, we have only 200 training samples. Let's try it:"
      ]
    },
    {
      "metadata": {
        "id": "U2w_j0wg4NAP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train a model with with an Embedding layer as before"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LiAzbP0h5lZ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Save the model weights to a file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrjPK_cO4NAS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot the performance of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XCwQQ4Ai4NAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Validation accuracy stalls in the low 50s. \n",
        "\n",
        "Finally, let's evaluate the model on the test data. First, we will need to tokenize the test data:"
      ]
    },
    {
      "metadata": {
        "id": "G5qeq-nW4NAV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZxGM12H29H6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Obtain the padded sequences for the test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SlVlYReN4NAX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And let's load and evaluate the first model:"
      ]
    },
    {
      "metadata": {
        "id": "7zDSs36Q4NAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the first model from the saved file\n",
        "#Reference - https://faroit.github.io/keras-docs/2.0.2/models/about-keras-models/\n",
        "\n",
        "#Evaluate against the test set\n",
        "#Reference - https://keras.io/models/model/#evaluate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rFEigRWOE1G0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets try with the second model:"
      ]
    },
    {
      "metadata": {
        "id": "TbXEVGKeE39J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load the second model from the saved file\n",
        "#Evaluate against the test set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "394siKPb4NAb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We get an appalling test accuracy of ~50% in both cases."
      ]
    },
    {
      "metadata": {
        "id": "jiFDp7m6ELXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## An alternate approach with TfIdf\n",
        "\n",
        "While using sequence vectors is one way transform our text data, and train a model, we can also use another approach by vectorizing the data using term frequencies across texts. Transforming text into Tfidf vectors is one such popular technique. Read more about TfIdf here: [https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
        "\n",
        "To convert our text to TfIdf vectors, we can use the popular Scikit learn library.\n",
        "\n",
        "Refer: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "metadata": {
        "id": "WhzGKxtkEUij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import the TfidfVectorizer from Sklearn and write a function to extract tfidf \n",
        "#  vectors from a given text\n",
        "\n",
        "def vectorize_using_tfidf(texts):\n",
        "  \"\"\"Returns Tfidf vectors of the given text along with the vectorizer\"\"\"\n",
        "  # TODO: fit texts to the tfidf vectorizer and return the vectorizer\n",
        "  pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wPyjEKR3E4ej",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can use sklearn's train_test_split function to convert your dataset into train and validation sets: \n",
        "\n",
        "Refer: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "metadata": {
        "id": "Z25T9iaJE3t2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#Load the dataset again\n",
        "texts, labels = load_imdb('./')  \n",
        "\n",
        "#Split the data into train and test\n",
        "x_train, x_val, y_train, y_val = train_test_split(texts, \n",
        "                                                  labels, \n",
        "                                                  test_size=5000, \n",
        "                                                  train_size=20000, \n",
        "                                                  random_state=42)\n",
        "\n",
        "# TODO: Output the shape of the new training data and test data\n",
        "\n",
        "# Lets vectorize x_train and x_val by calling vectorize_using_tfidf on x_train \n",
        "# and using the resulting vectorizer's transform method on x_val\n",
        "x_train, vectorizer = vectorize_using_tfidf(x_train)\n",
        "x_val = vectorizer.transform(x_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IJ-vZqSdF_-m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's use these vectors and train a simple neural network model and see how it performs on the validation set. "
      ]
    },
    {
      "metadata": {
        "id": "2lfgsdueF_x9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras.layers import Dropout, Dense\n",
        "\n",
        "def mlp_model(input_shape):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "    # Returns\n",
        "        An sequential model instance with 2 hidden layers and \n",
        "        dropout layers for regularization\n",
        "    \"\"\"\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=0.3, input_shape=input_shape))\n",
        "    # TODO: Add two dense layers with relu activation\n",
        "    #       and an output layer with sigmoid activation\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCBcnfzpGbSv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train our model using the tfidf vectors as our input:"
      ]
    },
    {
      "metadata": {
        "id": "SxmELi9RGgJU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create model instance.\n",
        "model = mlp_model(x_train.shape[1:]) \n",
        "\n",
        "# Use the adam optimzer from keras with a learning rate of 0.001\n",
        "# Refer: https://keras.io/optimizers/\n",
        "# TODO: Define an optimizer function\n",
        "\n",
        "# Create callback for early stopping on validation loss. If the loss does\n",
        "# not decrease in two consecutive tries, stop training.\n",
        "callbacks = [keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2)]\n",
        "\n",
        "# Compile the model with the optimizer and an appropriate loss function\n",
        "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['acc'])\n",
        "history = model.fit(\n",
        "          x_train,\n",
        "          y_train,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks,\n",
        "          validation_data=(x_val, y_val),\n",
        "          verbose=2,  # Logs once per epoch.\n",
        "          batch_size=32)\n",
        "\n",
        "# Print results.\n",
        "history = history.history\n",
        "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uv4qAcykJTbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, a simple model like this with TfIdf features goes a long way in getting a reasonable accuracy for such datasets with limited training examples. Let's evaluate this model on our test set and get some metrics."
      ]
    },
    {
      "metadata": {
        "id": "NOUCZPXcJZ1g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test, y_test = load_test_data('.')\n",
        "x_test = vectorizer.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "neiV2vsuJVqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll use sklearn's metrics functions to evaluate our model on our test set. "
      ]
    },
    {
      "metadata": {
        "id": "CC7KO42OJ2cc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "y_pred = model.predict_classes(x_test)\n",
        "print(y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8lrb2O1JJ5MM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = [item[0] for item in list(y_pred)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8y2ClHS0J7QQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S95s84je4NAc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using a Recurrent Neural Network in Keras"
      ]
    },
    {
      "metadata": {
        "id": "ZGY1DjYr4NAk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7nM08isR4NAy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`SimpleRNN` processes batches of sequences, like all other Keras layers, not just a single sequence.\n",
        "Like all recurrent layers in Keras, `SimpleRNN` can be run in two different modes: it can return either the full sequences of successive \n",
        "outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`), or it can return only the last output for each \n",
        "input sequence (a 2D tensor of shape `(batch_size, output_features)`). These two modes are controlled by the `return_sequences` constructor \n",
        "argument. Let's take a look at an example:"
      ]
    },
    {
      "metadata": {
        "id": "J09etMVl4NA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lets add a SimpleRNN layer to our model with embedding dimension 32"
      ]
    },
    {
      "metadata": {
        "id": "ngTLm0hB4NA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding layer of 10000 vocab size(or max features) and 32 dimensions\n",
        "# ...\n",
        "# Add a SimpleRNN layer of output 32 dimensions\n",
        "# ...\n",
        "\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kuJa104x4NBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. \n",
        "In such a setup, you have to get all intermediate layers to return full sequences:"
      ]
    },
    {
      "metadata": {
        "id": "S5UxZXlz4NBD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let us add 3 more SimpleRNN layers. This time we also want to set the \n",
        "# `return_sequences` parameter to be True.\n",
        "# This will return the output for each timestep as opposed to returning the output \n",
        "# for only the last timestep.\n",
        "# Compare model.summary() to see this difference.\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32))  # This last layer only returns the last outputs.\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVvoh3qy4NBG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's try to use such a model on the IMDB movie review classification problem. First, let's preprocess the data:"
      ]
    },
    {
      "metadata": {
        "id": "PSFJQhm94NBH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "max_features = 10000  # number of words to consider as features\n",
        "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "# Lets use more number of words for our dataset\n",
        "texts, labels = load_imdb('./')  \n",
        "data, labels, tokenizer = tokenize_texts(texts, labels, maxlen, max_features, remove_stopwords=True)\n",
        "((x_train, y_train), (x_val, y_val)) = split_data(data, labels, train_size=20000, validation_size=5000)\n",
        "\n",
        "texts, labels = load_test_data('.')\n",
        "x_test = pad_sequences(sequences, maxlen=200)\n",
        "y_test = np.asarray(labels)\n",
        "\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('input_train shape:', x_train.shape)\n",
        "print('input_test shape:', x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XCGz7Ltc4NBJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train a simple recurrent network using an `Embedding` layer and a `SimpleRNN` layer:"
      ]
    },
    {
      "metadata": {
        "id": "Nj3kROw74NBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bu4LlSrw4NBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's display the training and validation loss and accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "t5EWUc4B4NBM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJ1_kvCv4NBO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unfortunately, our small \n",
        "recurrent network doesn't perform very well at all compared to this baseline (only up to 85% validation accuracy). Part of the problem is \n",
        "that our inputs only consider the first 500 words rather the full sequences -- \n",
        "hence our RNN has access to less information than our earlier baseline model. The remainder of the problem is simply that `SimpleRNN` isn't very good at processing long sequences, like text. Other types of recurrent layers perform much better. Let's take a look at some \n",
        "more advanced layers."
      ]
    },
    {
      "metadata": {
        "id": "kOEdaiE-4NBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A concrete LSTM example in Keras\n",
        "Now let's switch to more practical concerns: we will set up a model using a LSTM layer and train it on the IMDB data. Here's the network, \n",
        "similar to the one with `SimpleRNN` that we just presented. We only specify the output dimensionality of the LSTM layer, and leave every \n",
        "other argument (there are lots) to the Keras defaults. Keras has good defaults, and things will almost always \"just work\" without you \n",
        "having to spend time tuning parameters by hand."
      ]
    },
    {
      "metadata": {
        "id": "pHEyGw6T4NBQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lets use an LSTM layer instead of a SimpleRNN layer"
      ]
    },
    {
      "metadata": {
        "id": "iwM-I9ny4NBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "# Add an Embedding layer as before with 10000 vocab size(max features) and 32 output dimensions\n",
        "# ...\n",
        "# Add a LSTM layer of 32 dimensions\n",
        "# ...\n",
        "\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mg6pJeXK4NBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zM3yajG74NBo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can see that the accuracy is ~87%, much higher than what we got with a SimpleRNN layer. "
      ]
    }
  ]
}